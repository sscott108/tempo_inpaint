{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d36f222-4464-403d-aa8f-f6ff2a1f9b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class STpconv3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=(3,3,3), stride=(1,1,1),\n",
    "                 use_bias=True, activation=None):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size if isinstance(kernel_size, tuple) else (kernel_size,)*3\n",
    "        self.stride = stride if isinstance(stride, tuple) else (stride,)*3\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        # Image kernel\n",
    "        self.weight = nn.Parameter(torch.empty(out_channels, in_channels, *self.kernel_size))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        if use_bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(out_channels))\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        # Activation\n",
    "        if activation == \"leaky\":\n",
    "            self.activation = nn.LeakyReLU(0.1, inplace=True)\n",
    "        elif activation == \"relu\":\n",
    "            self.activation = nn.ReLU(inplace=True)\n",
    "        elif activation == \"sigmoid\":\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif activation is None:\n",
    "            self.activation = None\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation: {activation}\")\n",
    "\n",
    "        # For mask convolution (all ones kernel)\n",
    "        self.register_buffer(\"mask_kernel\", torch.ones(out_channels, in_channels, *self.kernel_size))\n",
    "\n",
    "        # Padding\n",
    "        self.padding = tuple((k-1)//2 for k in self.kernel_size)\n",
    "\n",
    "    def forward(self, img, mask):\n",
    "        # Convolve mask (to count valid pixels per output location)\n",
    "        with torch.no_grad():\n",
    "            mask_out = F.conv3d(mask, self.mask_kernel, stride=self.stride, padding=self.padding)\n",
    "            mask_out = torch.clamp(mask_out, 0, 1)\n",
    "\n",
    "        # Convolve image * mask\n",
    "        img_out = F.conv3d(img * mask, self.weight, bias=None, stride=self.stride, padding=self.padding)\n",
    "\n",
    "        # Normalize by ratio\n",
    "        kernel_volume = self.kernel_size[0] * self.kernel_size[1] * self.kernel_size[2]\n",
    "        mask_ratio = kernel_volume / (mask_out + 1e-8)\n",
    "        mask_ratio = mask_ratio * mask_out  # zero where mask is zero\n",
    "        img_out = img_out * mask_ratio\n",
    "\n",
    "        if self.bias is not None:\n",
    "            img_out = img_out + self.bias.view(1, -1, 1, 1, 1)\n",
    "\n",
    "        if self.activation is not None:\n",
    "            img_out = self.activation(img_out)\n",
    "\n",
    "        return img_out, mask_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3bf5c89-5e4c-4d84-9319-934246fab76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STpconvUNet3D(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1,\n",
    "                 n_conv_layers=4, n_filters=None,\n",
    "                 kernel_sizes=None, strides=None,\n",
    "                 n_conv_per_block=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_conv_layers = n_conv_layers\n",
    "        self.n_conv_per_block = n_conv_per_block\n",
    "\n",
    "        if n_filters is None:\n",
    "            n_filters = [min(2**(i+5), 256) for i in range(n_conv_layers)]  # [32,64,...]\n",
    "\n",
    "        if kernel_sizes is None:\n",
    "            kernel_sizes = [(3,3,3)] * n_conv_layers\n",
    "\n",
    "        if strides is None:\n",
    "            strides = [(2,2,2)] * n_conv_layers\n",
    "\n",
    "        # Encoder\n",
    "        self.encoders = nn.ModuleList()\n",
    "        prev_ch = in_channels\n",
    "        for i in range(n_conv_layers):\n",
    "            block = nn.ModuleList()\n",
    "            for j in range(n_conv_per_block-1):\n",
    "                block.append(STpconv3D(prev_ch, n_filters[i], kernel_size=kernel_sizes[i],\n",
    "                                       stride=(1,1,1), activation=\"leaky\"))\n",
    "                prev_ch = n_filters[i]\n",
    "            block.append(STpconv3D(prev_ch, n_filters[i], kernel_size=kernel_sizes[i],\n",
    "                                   stride=strides[i], activation=\"leaky\"))\n",
    "            self.encoders.append(block)\n",
    "            prev_ch = n_filters[i]\n",
    "\n",
    "        # Decoder\n",
    "        self.decoders = nn.ModuleList()\n",
    "        for i in reversed(range(n_conv_layers)):\n",
    "            block = nn.ModuleList()\n",
    "            nf = out_channels if i == 0 else n_filters[i-1]\n",
    "            block.append(STpconv3D(prev_ch + (in_channels if i==0 else n_filters[i-1]),\n",
    "                                   nf, kernel_size=(3,3,3), stride=(1,1,1), activation=\"leaky\"))\n",
    "            prev_ch = nf\n",
    "            self.decoders.append(block)\n",
    "\n",
    "        # Final 1x1x1 conv\n",
    "        self.final = nn.Conv3d(prev_ch, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, img, mask):\n",
    "        # Encoder forward\n",
    "        skips = []\n",
    "        skip_masks = []\n",
    "        x, m = img, mask\n",
    "        for block in self.encoders:\n",
    "            for layer in block:\n",
    "                x, m = layer(x, m)\n",
    "            skips.append(x)\n",
    "            skip_masks.append(m)\n",
    "\n",
    "        # Decoder forward\n",
    "        for i, block in enumerate(self.decoders):\n",
    "            x = F.interpolate(x, scale_factor=2, mode=\"trilinear\", align_corners=False)\n",
    "            m = F.interpolate(m, scale_factor=2, mode=\"nearest\")\n",
    "\n",
    "            if i < self.n_conv_layers:\n",
    "                e = skips[self.n_conv_layers-1-i-1] if (self.n_conv_layers-1-i-1)>=0 else img\n",
    "                em = skip_masks[self.n_conv_layers-1-i-1] if (self.n_conv_layers-1-i-1)>=0 else mask\n",
    "                x = torch.cat([x, e], dim=1)\n",
    "                m = torch.cat([m, em], dim=1)\n",
    "\n",
    "            for layer in block:\n",
    "                x, m = layer(x, m)\n",
    "\n",
    "        return self.final(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18755e1-9c01-4af1-83d5-b2db2a8da4ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
